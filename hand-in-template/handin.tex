%	Copyright (C) 2013 Systems Engineering Group
%
%	CHANGELOG:
%       2005-10-10 - corrected and extended. 
%       2013-01-28 - adjusted sections and explanation
%		2020-11-18 - basic content of hand-in


\documentclass[a4paper,10pt,twoside]{article}
\pagestyle{headings}
\usepackage{a4wide}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{csquotes}
\graphicspath{ {./images/} }
\usepackage[colorlinks,hyperfigures,backref,bookmarks,draft=false]{hyperref}

\title{Disaggregated Compute and Storage for Distributed LSM-based Databases}
\author{Zidi Chen}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Here you provide a very short intoduction into your topic and sum up your hand-in. 
It is important to highlight the main issues to be discussed in your hand-in here.
\end{abstract}

\tableofcontents

\section{Introduction of the Research Field}

Distributed databases like MongoDB, TiDB have been more widely used in many large-scale services senarios.
But in many cases, distributed databases could suffer performace degradation and low utilizationn from skew, backgroud operatio and compaction.
The paper~\textit{Hailstorm: Disaggregated Compute and Storage for Distributed LSM-based Databases} \cite{mainpaper}introduces Hailstorm, a system that disaggregates storage and compute for distributed LSM-based databases.
It shows that Hailstorm achieves load balance in many MongoDB deployments with skewed workloads, improving the average throughput by 60$\%$, while decreasing tail latency by as much as 5Ã—.


\section{Basics}

\subsection{LSM-Tree}

Traditional disk-based index structures such as the B-tree will effectively double the I/O cost of the transaction to maintain an index such as this in real time, increasing the total system cost up to fifty percent. 
Hence Log-structured merge tree (LSM-Tree) \cite{LSM-tree} has been widely used by many distributed databases, as it provides efficient indexing for a key-value store with a high rate of inserts and deletes.
The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort. 
During this process all index values are continuously accessible to retrievals (aside from very short locking periods), either through the memory component or one of the disk components.

\subsection{Compaction in LSM KV Stores}

Compaction is a critical mechanism in a system based on LSM-Tree. 
Log append method brings high-throughput writes. 
As the sstable continues to be written, more and more files will be opened by the system, and the accumulated data changes (updates, deletes) operations for the same key will increase. 
Since sstable is immutable, the data in the memory will reaches the upper limit in certain layer.
In order to reduce the number of files and clean up invalid data in time, the compaction mechanism was introduced to optimize read performance and space.
\par
Compaction can be implemented in many ways. For example, RocksDB implements Tiered+Leveled, termed Level compaction.\cite{RocksDB_compaction_algo1}. 
\par
Generally, in LSM tree, each level contains multiple partitions of data sorted by keys. When  compaction will be triggered by continuously recycling old version data and merging multiple layers into one layer through periodic background tasks.
The basic procedure goes as following:
\begin{figure}[h]
    \centering
	\includegraphics[scale=0.3]{LSM_Tree1.png}
    \caption{LSM Tree}
    \label{fig:mesh1}
\end{figure}
\par
As is shown in Figure. Compaction into level N (L(n)) merges data from L(n-1) to L(n), where data is rewirted into the new level. 
In original paper of LSM tree, all data from L(n-1) is merged into L(n). 
In morden distributed databases such as LevelDB and RocksDB, overlapping data is eliminated, therefore for most time, only some data will be merged into new level.
\par
Generally, LSM tree will continuously compact key-value pairs ,and the compaction and disk flushes will trigger big writes, which will cause significantly overheads due to I/O amplifications.


\subsection{Tow-level Sharding}
Sharding is widely used in distributed databases. Large data sets or high throughput applications can challenge the capacity of one single server.
Each area has the same schema and columns, but each table has completely different rows. 
Similarly, the data stored in each partition is unique and has nothing to do with the data stored in other partitions.
Sharding divides a piece of data into two or more smaller blocks, called logical shards.
Then, logical shards are distributed on separate database nodes, called physical shards.
Physical shards can hold multiple logical shards. 
Nevertheless, the data stored in all shards collectively represent the entire logical data set.
\par
A tow-level sharding way is introduced in the paper\cite{mainpaper}. 
Hailstorm uses a tow-step data assignment scheme, where data objects are dynamically assigned to different partitions, 
so that the system could react better to the changes in the workfload and perform better load balance.
The target LSM-based databases still use the filesystem solution to redistribute data blocks into different nodes uniformly.


\subsection{Skew in Distributed Databases}

Distributed databases \cite{mongodb,tidb} are designed for large-scale data storage, which usually run on multiple nodes in the same,
or even partitioned networks.
Distributed database use sharding to tore larger dataset and handle additional requests by distributing the data among multiple machines.
The database engine translates user queries into individual queries that are routed to one or multiple database instances for execution.
Therefore, the database may suffer skew issue, where keys are unevenly to different nodes, which could cause uneven access and some nodes may become the hotsopt.
This could lead to globally performance degradation.



\section{Previous and Related Work}

\subsection{Skew in Distributed Databases}

Previously proposed solutions to eliminate skew in several ways.
MongoDB introduces a balancer to mitigate imbalanced shards\cite{MongoDB_balancing1}.
Some manual operations can also be performed, e.g., moving "hotspot" chunks manually to address the imbalances. 
In practice, some table related operations will also work, e.g., spliting one table into several sub-tables.
\par
Resharding could be carried out manually or automatically to reduce the influence on performance due to skew.
Auto-splitting is a mechanism in MongoDB\cite{MongoDB_sharding1} that detect automatically when data overflow happens in chunks, 
and then split the oversized chunk into smaller pieces.
The server that runs MongoDB contains the process ChunkSplitter that runs continuously to detect weather any chunk grows beyond the configured chunk size (maxChunkSizeBytes) due to some insert operation.
\par
The auto split task will be carried out if possible and necessary. ChunkSplitter supports asynchronous auto-split, thus, 
multiple split operations could be handled concurrently with no overlap. 
Basic proceduce goes as following: 
First, ChunkSplitter will set the flag to the chunks which need resharding, where the estimated data size will be reset to handle the potiential in-coming writing operations.
Then the splitVector will perform the split by inserting the split keys to original chunk vector, and returning the split points.
\par
Auto-split may not be enough to handle skew issue, as it requires additional operations to reduce read operation hot spots.
So MongoDB uses top chunk optimization to prevent some top chunks becoming potential hot spot.
The balancer ties to move the top chunk out of the shart when auto-split is carried out.
MongoDB also provides auto-balancing mechanism that try to reduce hot spot chunks, where a balancer process runs continuously in the backgroud to monitor the chunk distribution in the cluster.
Resharding operation may still cause some performance loss, some manual operations that takes the structure of the table in consideration may be costly in time.

\subsection{Compaction and performance degradation}
Compaction may cause severe I/O performance degradation in LSM-based distributed databases, 
as it merges multiple tables and writes back to disk, which will typically comsue large I/O and CPU computing resources.
A benchmark test is conducted in the paper\cite{mainpaper}, 
\begin{displayquote}
	Profiling this particular experiment
	reveals that storage is saturated as I/O bandwidth remains
	almost constantly close to 320 MB/s, the maximum write
	bandwidth for our SSD. We also observe peaks of CPU utilization
	when compaction tasks run. 
\end{displayquote}
\begin{figure}[h]
    \centering
	\includegraphics[scale=0.3]{Campaction perf.png}
    \caption{Embedded RocksDB throughput over time (HH:mm) on
	a node equipped with an Intel S3500 Series SSD using the YCSB
	A [38] workload (50$\%$ reads and 50$\%$ writes).}
    \label{fig:mesh1}
\end{figure}

\subsubsection{Light-weight LSM Tree}
The light-weight compaction tree(LWC-Tree) \cite{Yao2017ALC} is a variant from the original LSM tree. 
The LWC-Tree tries to improve the write preformance during compaction by merging a little metadata rather than perform the operation on the while range of data.
As is discussed previously, traditional lSM tree carries out compaction by read, sort and rewrite all whole tables. 
\par
LWC-Tree divides the overlapping data from the original table into small segments, and then appends these segments into the tables on the next level, 
and then only merge related metadata, which is a very small scale compared to the original scale. 
LWC-Tree organizes the DTable structure based on the change of metadata. 
LWC-Tree also provides load balancing for the same level DTables to reduce hot spots as much as possible. 
The balancer moves the overly-full tables data to other tables on the same level by adjusting the key range, which reduces the cost for data movement as much as possible.


\subsubsection{Reducing Involved Components in Compaction}
Skip-tree \cite{7569086} solve the performance degradation caused by compaction in a quite aggresive way.
As is discussed, I/O performance gets severe influence by the compaction due to heavily read and write-to-disk operations.
So Skip-tree skips some components in compaction and put the output of the compaction into some larger components.
The basic architecture of Skip-tree is shown in Figure 3.
Skip-tree introduces a large size memory to store the index for the KV objects as a buffer.
It push the KV objects into non-adjacent components via skipping components during the precedure of compaction as many as possible.
This would reduce the steps of operations from memory to larger disk components, which reduces the I/O throughput.
As a result, there will be fewer compactions before the KV objects reach the destination component.
\begin{figure}[h]
    \centering
	\includegraphics[scale=0.3]{Skip-tree.png}
    \caption{ The architecture of Skip-tree.}
    \label{fig:mesh1}
\end{figure}

\subsubsection{Disk Side optimization}
SMRDB \cite{SMRDB} is a KV data store that works specially for disks to address the need that to utilize disks by using approaches to leverage their proclivity for sequential writes as much as possible.
It aims at providing a new key-value data management for SMR disks to handle the overlapping data in random write and in-place updates operations on the disk.
SMRDB works as a KV database engine on the SMR disks. 
\par
The basic idea of SMR solution for LSM Tree is shown in Figure 4.
\begin{figure}[h]
    \centering
	\includegraphics[scale=0.3]{SMR.png}
    \caption{ LevelDB based data access management.}
    \label{fig:mesh1}
\end{figure}
To meet the requirement of large data reorganization in LSM-Tree based databases, a simple SMR disks solution is introduced in this paper.
Stitching is used in the compaction process, where it basicly moves part of the compaction data to a new area, 
but some splited data will still remain in the same place and be stitched with in-coming new data.
But it can't be evaluated how this scheme works when compactions are carried in the background.



\subsection{Summary}
It can be shown from the previous work that skew will cause severe performance degradation, and the database system suffer from I/O brusts due to compaction and disk flushing.
MongoDB's solution to reduce skew is basically based on resharding and monitoring mechanism.
There are also many solutions that try to slove the I/O and CPU perfromance degradation.
But these three solutions above are mostly aimed at LSM-tree structure on individual nodes, they don't take the utilization of other nodes in consideration.
Hailstrom tries to solve these problems by disaggregate resources to address load balancing at database and storage layers seperately.

\section{Design of Hailstrom: Disaggregate Compute and Storage}

Hailstorm is designed to improve load balance and utilization for LSM-based distributed database.
It on a higher level to reduce the skew for read and write operations, as well as to reduce the I/O and CPU performace issue caused by compaction behavior.
The key idea of Hailstorm is to disaggregate compute and storage.

\subsection{Basic Architecture}
The basic architecture of Hailstorm shown in Figure 5. 
Hailstorm works like a middleware in the middle of in-memory database storage instances and disk side storage devices.
The Hailstorm filesystem contains three parts:
1. Clients that provide filesystem interface to storage engines,
2. Servers that operate on local storages to store data and relavant operations,
3. Hailstorm agent in the middle to schedule operations of compaction tasks.

\begin{figure}[h]
    \centering
	\includegraphics[scale=0.3]{Hailstorm2.png}
    \caption{ Distributed database deployed on a Hailstorm architecture.
	Hailstorm spreads data uniformly for each storage engine
	across all pooled storage devices within the rack.}
    \label{fig:mesh1}
\end{figure}

The user side operations could still operate on traditional databases. 

\subsection{Hailstorm Filesystem Interface}
Hailstorm provides a subset of POSIX filesystem interface to storage engines rather than block-level interface.
As Hailstorm in the middle of local storage and database nodes, filesystem interface could provide filesystem-level informantion to sotrage engines.
Therefore Hailstorm sheilds KV store related specialized implementation details, and exposes traditional POSIX interface to nodes.
\par
In order to promote I/O performance, Hailstorm filesystem keeps most metadata locally and use smaller block size compared with traditional LSM-based databases.
The filesystem is also designed specially to reduce the influence of compaction, and aggresive prefetching is carried out to compaction tasks.

\subsection{Storage Architecture Design}




\section{Evaluation}

Present and discuss measurements, experiments, examples, ... but do not repeat the entire evaluation of the original paper.
\emph{Cite} all figures and tables copied form other papers.
You can keep this section short and focus on the aspects that were improved by the paper compared to existing approaches.

\section{Discussion}

What is bad about your paper? 
What are the good points? 
Mention criticism and ideas for improvement that you thought about while researching the topic.

\section{Conclusion}

Sum up your paper and the discussion points.

\bibliographystyle{unsrt}
\bibliography{handin} 

\end{document}

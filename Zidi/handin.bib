@article{mainpaper,
  title = {Hailstorm: Disaggregated Compute and Storage for Distributed LSM-based Databases},
  author = {Laurent Bindschaedler, Ashvin Goel, Willy Zwaenepoel},
  journal = {ASPLOS’20},
  year = {2020},
  month = {March},
  pages = {301–316}
}
@article{LSM-tree,
author = {O'Neil, Patrick and Cheng, Edward and Gawlick, Dieter and O'Neil, Elizabeth},
title = {The Log-Structured Merge-Tree (LSM-Tree)},
year = {1996},
issue_date = {1996},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0001-5903},
url = {https://doi.org/10.1007/s002360050048},
doi = {10.1007/s002360050048},
journal = {Acta Inf.},
month = jun,
pages = {351–385},
numpages = {35}
}
@ARTICLE{7569086,  author={Y. {Yue} and B. {He} and Y. {Li} and W. {Wang}},  journal={IEEE Transactions on Parallel and Distributed Systems},   title={Building an Efficient Put-Intensive Key-Value Store with Skip-Tree},   year={2017},  volume={28},  number={4},  pages={961-973},  doi={10.1109/TPDS.2016.2609912}}
@ARTICLE{10.1145/3033273,
author = {Lu, Lanyue and Pillai, Thanumalayan Sankaranarayana and Gopalakrishnan, Hariharan and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
title = {WiscKey: Separating Keys from Values in SSD-Conscious Storage},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1553-3077},
url = {https://doi.org/10.1145/3033273},
doi = {10.1145/3033273},
abstract = {We present WiscKey, a persistent LSM-tree-based key-value store with a performance-oriented data layout that separates keys from values to minimize I/O amplification. The design of WiscKey is highly SSD optimized, leveraging both the sequential and random performance characteristics of the device. We demonstrate the advantages of WiscKey with both microbenchmarks and YCSB workloads. Microbenchmark results show that WiscKey is 2.5 \texttimes{} to 111 \texttimes{} faster than LevelDB for loading a database (with significantly better tail latencies) and 1.6 \texttimes{} to 14 \texttimes{} faster for random lookups. WiscKey is faster than both LevelDB and RocksDB in all six YCSB workloads.},
journal = {ACM Trans. Storage},
month = mar,
articleno = {5},
numpages = {28},
keywords = {flash-based SSDs, WiscKey, LevelDB}
}
@inproceedings{10.5555/2813767.2813783,
author = {Marmol, Leonardo and Sundararaman, Swaminathan and Talagala, Nisha and Rangaswami, Raju},
title = {NVMKV: A Scalable, Lightweight, FTL-Aware Key-Value Store},
year = {2015},
isbn = {9781931971225},
publisher = {USENIX Association},
address = {USA},
abstract = {Key-value stores are ubiquitous in high performance data-intensive, scale out, and NoSQL environments. Many KV stores use flash devices for meeting their performance needs. However, by using flash as a simple block device, these KV stores are unable to fully leverage the powerful capabilities that exist within Flash Translation Layers (FTLs). NVMKV is a lightweight KV store that leverages native FTL capabilities such as sparse addressing, dynamic mapping, transactional persistence, and support for high-levels of lock free parallelism. Our evaluation of NVMKV demonstrates that it provides scalable, high-performance, and ACID compliant KV operations at close to raw device speeds.},
booktitle = {Proceedings of the 2015 USENIX Conference on Usenix Annual Technical Conference},
pages = {207–219},
numpages = {13},
location = {Santa Clara, CA},
series = {USENIX ATC '15}
}
@inproceedings{Yao2017ALC,
  title={A Light-weight Compaction Tree to Reduce I / O Amplification toward Efficient Key-Value Stores},
  author={T. Yao and Jiguang Wan and P. Huang and Xubin He and Q. Gui and F. Wu and C. Xie},
  year={2017}
}
@misc{mongodb,
  title        = {MongoDB},
  howpublished = {\url{https://www.mongodb.com/}}
}
@misc{tidb,
  author       = {PingCAP},
  title        = {TiDB},
  howpublished = {\url{https://pingcap.com/}},
}
@misc{RocksDB_compaction_algo1,
  author       = {RocksDB},
  title        = {Overview of Compaction algorithm},
  howpublished = {\url{https://github.com/facebook/rocksdb/wiki/Compaction}},
}
@misc{MongoDB_balancing1,
  author       = {MongoDB},
  title        = {Sharding Internals},
  howpublished = {\url{https://www.mongodb.com/blog/post/sharding-pitfalls-part-iii-chunk-balancing-and}},
}
@misc{MongoDB_sharding1,
  author       = {MongoDB},
  title        = {Chunk Balancing and Collection Limits},
  howpublished = {\url{https://github.com/mongodb/mongo/blob/master/src/mongo/db/s/README.md}},
}
@inproceedings{SMRDB,
author = {Pitchumani, Rekha and Hughes, James and Miller, Ethan},
year = {2015},
month = {05},
pages = {},
title = {SMRDB: Key-Value Data Store for Shingled Magnetic Recording Disks},
doi = {10.1145/2757667.2757680}
}
@article{10.1145/369275.369291,
author = {Poess, Meikel and Floyd, Chris},
title = {New TPC Benchmarks for Decision Support and Web Commerce},
year = {2000},
issue_date = {Dec. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/369275.369291},
doi = {10.1145/369275.369291},
abstract = {For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.},
journal = {SIGMOD Rec.},
month = dec,
pages = {64–71},
numpages = {8}
}
@article{10.1145/1942776.1942778,
author = {Chen, Shimin and Ailamaki, Anastasia and Athanassoulis, Manos and Gibbons, Phillip B. and Johnson, Ryan and Pandis, Ippokratis and Stoica, Radu},
title = {TPC-E vs. TPC-C: Characterizing the New TPC-E Benchmark via an I/O Comparison Study},
year = {2011},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/1942776.1942778},
doi = {10.1145/1942776.1942778},
abstract = {TPC-E is a new OLTP benchmark recently approved by the Transaction Processing Performance Council (TPC). In this paper, we compare TPC-E with the familiar TPCC benchmark in order to understand the behavior of the new TPC-E benchmark. In particular, we compare the I/O access patterns of the two benchmarks by analyzing two OLTP disk traces. We find that (i) TPC-E is more read intensive with a 9.7:1 I/O read to write ratio, while TPC-C sees a 1.9:1 read-to-write ratio; and (ii) although TPC-E uses pseudo-realistic data, TPC-E's I/O access pattern is as random as TPC-C. The latter suggests that like TPC-C, TPC-E can benefit from SSDs, which have superior random I/O support. To verify this, we replay both disk traces on an Intel X25-E SSD and see dramatic improvements for both TPC-C and TPC-E.},
journal = {SIGMOD Rec.},
month = feb,
pages = {5–10},
numpages = {6}
}